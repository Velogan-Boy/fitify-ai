{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-02 15:01:35.129311: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-04-02 15:01:35.808819: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-04-02 15:01:35.808943: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-04-02 15:01:35.900626: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-04-02 15:01:36.258979: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-04-02 15:01:36.278269: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-02 15:01:41.603686: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import time\n",
    "import psutil\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (LSTM, Dense, Dropout, Input, Flatten, \n",
    "                                     Bidirectional, Permute, multiply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1712050306.827484 1026507 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1712050306.880132 1027641 gl_context.cc:344] GL version: 3.2 (OpenGL ES 3.2 Mesa 23.0.4-0ubuntu1~22.04.1), renderer: Mesa Intel(R) UHD Graphics 620 (KBL GT2)\n"
     ]
    }
   ],
   "source": [
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose(static_image_mode=False, min_detection_confidence=0.5, min_tracking_confidence=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints(results):\n",
    "    \"\"\"\n",
    "    Processes and organizes the keypoints detected from the pose estimation model \n",
    "    to be used as inputs for the exercise decoder models\n",
    "    \n",
    "    \"\"\"\n",
    "    pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten()\n",
    "\n",
    "    return pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_block(inputs, time_steps):\n",
    "    \"\"\"\n",
    "    Attention layer for deep neural network\n",
    "    \n",
    "    \"\"\"\n",
    "    # Attention weights\n",
    "    a = Permute((2, 1))(inputs)\n",
    "    a = Dense(time_steps, activation='softmax')(a)\n",
    "    \n",
    "    # Attention vector\n",
    "    a_probs = Permute((2, 1), name='attention_vec')(a)\n",
    "    \n",
    "    # Luong's multiplicative score\n",
    "    output_attention_mul = multiply([inputs, a_probs], name='attention_mul') \n",
    "    \n",
    "    return output_attention_mul\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(HIDDEN_UNITS=256, sequence_length=30, num_input_values=33*4, num_classes=3):\n",
    "    \"\"\"\n",
    "    Function used to build the deep neural network model on startup\n",
    "\n",
    "    Args:\n",
    "        HIDDEN_UNITS (int, optional): Number of hidden units for each neural network hidden layer. Defaults to 256.\n",
    "        sequence_length (int, optional): Input sequence length (i.e., number of frames). Defaults to 30.\n",
    "            num_input_values (_type_, optional): Input size of the neural network model. Defaults to 33*4 (i.e., number of keypoints x number of metrics).\n",
    "            num_classes (int, optional): Number of classification categories (i.e., model output size). Defaults to 3.\n",
    "\n",
    "    Returns:\n",
    "        keras model: neural network with pre-trained weights\n",
    "    \"\"\"\n",
    "    # Input\n",
    "    inputs = Input(shape=(sequence_length, num_input_values))\n",
    "    # Bi-LSTM\n",
    "    lstm_out = Bidirectional(LSTM(HIDDEN_UNITS, return_sequences=True))(inputs)\n",
    "    # Attention\n",
    "    attention_mul = attention_block(lstm_out, sequence_length)\n",
    "    attention_mul = Flatten()(attention_mul)\n",
    "    # Fully Connected Layer\n",
    "    x = Dense(2*HIDDEN_UNITS, activation='relu')(attention_mul)\n",
    "    x = Dropout(0.5)(x)\n",
    "    # Output\n",
    "    x = Dense(num_classes, activation='softmax')(x)\n",
    "    # Bring it all together\n",
    "    model = Model(inputs=[inputs], outputs=x)\n",
    "\n",
    "    ## Load Model Weights\n",
    "    load_dir = \"../models/LSTM.h5\"  \n",
    "    model.load_weights(load_dir)\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIDENCE_THRESHOLD = 0.8\n",
    "SEQUENCE_LENGTH = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model()\n",
    "\n",
    "cap = cv2.VideoCapture('../dataset/squat/squat_11.mp4')\n",
    "# cap = cv2.VideoCapture('../dataset/hammer curl/hammer curl_11.mp4')\n",
    "# cap = cv2.VideoCapture(0)\n",
    "\n",
    "# cap = cv2.VideoCapture('../dataset/barbell biceps curl/barbell biceps curl_11.mp4')\n",
    "\n",
    "\n",
    "sequence = []\n",
    "actions = np.array(['curl', 'press', 'squat'])\n",
    "current_action = ''\n",
    "\n",
    "prev_frame_time = 0\n",
    "new_frame_time = 0\n",
    "\n",
    "while cap.isOpened():\n",
    "      ret, frame = cap.read()\n",
    "    \n",
    "      if not ret: \n",
    "            break\n",
    "      \n",
    "      rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "      results = pose.process(rgb_frame)\n",
    "    \n",
    "      if results.pose_landmarks:\n",
    "            landmarks = extract_keypoints(results)\n",
    "            mp.solutions.drawing_utils.draw_landmarks(frame, results.pose_landmarks, mp_pose.POSE_CONNECTIONS)\n",
    "      else: \n",
    "            continue \n",
    "      \n",
    "      sequence.append(landmarks)\n",
    "      \n",
    "      sequence = sequence[-SEQUENCE_LENGTH:]\n",
    "      \n",
    "      if len(sequence) == SEQUENCE_LENGTH:\n",
    "            res = model.predict(np.expand_dims(sequence[-30:], axis=0), verbose=0)[0]\n",
    "            \n",
    "            current_action = actions[np.argmax(res)]\n",
    "            confidence = np.max(res)\n",
    "            \n",
    "            if (confidence < 0.8):\n",
    "                  current_action = ''\n",
    "      \n",
    "      # CPU analysis\n",
    "      new_frame_time = time.time() \n",
    "\n",
    "      fps = 1/(new_frame_time-prev_frame_time) \n",
    "      prev_frame_time = new_frame_time \n",
    "      fps = str(int(fps)) \n",
    "\n",
    "      ram_usage = psutil.virtual_memory().percent\n",
    "      cpu_usage = psutil.cpu_percent()\n",
    "\n",
    "      # putting the FPS count on the frame \n",
    "      cv2.putText(frame, 'FPS: {}'.format(fps), (1000, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (100, 255, 0), 2) \n",
    "\n",
    "      # Display the RAM usage\n",
    "      cv2.putText(frame, f\"RAM Usage: {ram_usage}%\", (1000, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (100, 255, 0), 2)\n",
    "\n",
    "      # Display CPU usage\n",
    "      cv2.putText(frame, f\"CPU Usage: {cpu_usage}%\", (1000, 90), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (100,255,0), 2)\n",
    "      \n",
    "      cv2.rectangle(frame, (0,0), (360, 40), 0.5, -1)\n",
    "      cv2.putText(frame, 'Exercise ' + current_action, (3,30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "    \n",
    "\n",
    "      cv2.imshow('Output Video', frame)\n",
    "\n",
    "      if cv2.waitKey(10) & 0xFF==ord('q'):\n",
    "            break\n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
