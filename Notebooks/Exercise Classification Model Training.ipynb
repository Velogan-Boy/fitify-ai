{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d602e75d",
   "metadata": {},
   "source": [
    "# 0. Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d97e5ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-21 14:54:27.874532: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-04-21 14:54:27.920080: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-04-21 14:54:27.920124: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-04-21 14:54:27.921729: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-04-21 14:54:27.930514: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-04-21 14:54:27.931342: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-21 14:54:28.867903: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import mediapipe as mp\n",
    "import tensorflow as tf\n",
    "import math\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import multilabel_confusion_matrix, accuracy_score, classification_report\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.callbacks import TensorBoard, EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "\n",
    "from tensorflow.keras.layers import (LSTM, Dense, Concatenate, Attention, Dropout, Softmax,\n",
    "                                     Input, Flatten, Activation, Bidirectional, Permute, multiply, \n",
    "                                     ConvLSTM2D, MaxPooling3D, TimeDistributed, Conv2D, MaxPooling2D)\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "# disable some of the tf/keras training warnings \n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = \"3\"\n",
    "tf.get_logger().setLevel(\"ERROR\")\n",
    "tf.autograph.set_verbosity(1)\n",
    "\n",
    "# suppress untraced functions warning\n",
    "import absl.logging\n",
    "absl.logging.set_verbosity(absl.logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01ec993",
   "metadata": {},
   "source": [
    "# 5. Preprocess Data and Create Labels/Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb9e4f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = np.array(['barbell biceps curl','bench press','chest fly machine','deadlift','decline bench press',\n",
    "                  'hammer curl','hip thrust','incline bench press','lat pulldown','lateral raise',\n",
    "                  'leg extension','leg raises','plank','pull Up','push-up','romanian deadlift',\n",
    "                  'russian twist','shoulder press','squat','t bar row','tricep Pushdown','tricep dips'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cad528c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {label:num for num, label in enumerate(actions)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0149b8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0add3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and organize recorded training data\n",
    "sequences, labels = [], []\n",
    "result_dir=\"WVE\"\n",
    "for action in actions:\n",
    "    output_dir = os.path.join(result_dir, action)\n",
    "    result_file_path_x = output_dir + \"+X.txt\"\n",
    "\n",
    "    videos_data = []\n",
    "    with open(result_file_path_x, \"rb\") as filex:\n",
    "      for inpx in pickle.load(filex):\n",
    "        videos_data.append(inpx)\n",
    "    \n",
    "    for video_data in videos_data:\n",
    "        sequences.append(video_data)\n",
    "        labels.append(label_map[action])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88280539",
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = sequences[0]\n",
    "landmark_obj = s1[0]\n",
    "for lo in landmark_obj:\n",
    "    lo.x = 0\n",
    "    lo.y = 0\n",
    "    lo.z = 0\n",
    "    lo.visibility = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e9175b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_seq_length = 250\n",
    "num_landmarks = 33\n",
    "\n",
    "sequences = [seq[:len(seq)] + [landmark_obj]  * (fixed_seq_length - len(seq)) if len(seq) < fixed_seq_length else seq[:fixed_seq_length] for seq in sequences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0984dc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints(results):\n",
    "    \"\"\"\n",
    "    Processes and organizes the keypoints detected from the pose estimation model \n",
    "    to be used as inputs for the exercise decoder models\n",
    "    \n",
    "    \"\"\"\n",
    "    pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results]).flatten()\n",
    "    return pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab0eb61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = [[extract_keypoints(s) for s in seq] for seq in sequences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "563271ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 40\n",
    "\n",
    "def split_sequence(sequence, label, new_sequences, new_labels):\n",
    "    for i in range(0, len(sequence), sequence_length):\n",
    "    # Check if the remaining elements are less than the partition_length\n",
    "        if len(sequence[i:]) >= sequence_length:\n",
    "            new_sequences.append(sequence[i:i + sequence_length])\n",
    "            new_labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a5c5d320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "652\n",
      "652\n"
     ]
    }
   ],
   "source": [
    "print(len(sequences))\n",
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fabcd1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sequences, new_labels = [], []\n",
    "for sequence, label in zip(sequences, labels):\n",
    "    split_sequence(sequence, label, new_sequences, new_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cc2f0086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3912\n",
      "3912\n"
     ]
    }
   ],
   "source": [
    "print(len(new_sequences))\n",
    "print(len(new_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7fc71e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = new_sequences\n",
    "labels = new_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ab459ce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3912, 40, 132) (3912, 22)\n"
     ]
    }
   ],
   "source": [
    "# Make sure first dimensions of arrays match\n",
    "X = np.array(sequences)\n",
    "y = to_categorical(labels).astype(int)\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5ac49993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3520, 40, 132) (3520, 22)\n"
     ]
    }
   ],
   "source": [
    "# Split into training, validation, and testing datasets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.10, random_state=1)\n",
    "print(X_train.shape, y_train.shape)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=15/90, random_state=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53ae03d",
   "metadata": {},
   "source": [
    "# 6. Build and Train Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a188a4bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/dell/Desktop/fyp/HP/data\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = os.path.join(os. getcwd(),'data') \n",
    "print(DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "912f3153",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "\n",
    "# some hyperparamters\n",
    "batch_size = 32\n",
    "max_epochs = 40"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ca0cad",
   "metadata": {},
   "source": [
    "## 6a. LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "25556d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Videos are going to be this many frames in length\n",
    "\n",
    "\n",
    "num_input_values = num_landmarks * 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ae7595c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 40, 128)           133632    \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 40, 256)           394240    \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 128)               197120    \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 22)                1430      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 751190 (2.87 MB)\n",
      "Trainable params: 751190 (2.87 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "lstm = Sequential()\n",
    "lstm.add(LSTM(128, return_sequences=True, activation='relu', input_shape=(sequence_length, num_input_values)))\n",
    "lstm.add(LSTM(256, return_sequences=True, activation='relu'))\n",
    "lstm.add(LSTM(128, return_sequences=False, activation='relu'))\n",
    "lstm.add(Dense(128, activation='relu'))\n",
    "lstm.add(Dense(64, activation='relu'))\n",
    "lstm.add(Dense(actions.shape[0], activation='softmax'))\n",
    "print(lstm.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8a10e698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "92/92 [==============================] - 13s 108ms/step - loss: 4261619.5000 - categorical_accuracy: 0.0641 - val_loss: 3.0111 - val_categorical_accuracy: 0.0903\n",
      "Epoch 2/40\n",
      "92/92 [==============================] - 10s 108ms/step - loss: 235.5587 - categorical_accuracy: 0.0794 - val_loss: 2.9450 - val_categorical_accuracy: 0.0954\n",
      "Epoch 3/40\n",
      "92/92 [==============================] - 10s 112ms/step - loss: 43.2028 - categorical_accuracy: 0.1043 - val_loss: 2.8946 - val_categorical_accuracy: 0.0954\n",
      "Epoch 4/40\n",
      "92/92 [==============================] - 10s 113ms/step - loss: 2.9071 - categorical_accuracy: 0.1108 - val_loss: 2.8902 - val_categorical_accuracy: 0.1175\n",
      "Epoch 5/40\n",
      "92/92 [==============================] - 11s 116ms/step - loss: 2437.1882 - categorical_accuracy: 0.1074 - val_loss: 2.9798 - val_categorical_accuracy: 0.0733\n",
      "Epoch 6/40\n",
      "92/92 [==============================] - 11s 116ms/step - loss: 348.1482 - categorical_accuracy: 0.0951 - val_loss: 2.9626 - val_categorical_accuracy: 0.0818\n",
      "Epoch 7/40\n",
      "92/92 [==============================] - 11s 116ms/step - loss: 2.9527 - categorical_accuracy: 0.0985 - val_loss: 2.9609 - val_categorical_accuracy: 0.0818\n",
      "Epoch 8/40\n",
      "92/92 [==============================] - 11s 119ms/step - loss: 2.9514 - categorical_accuracy: 0.0985 - val_loss: 2.9596 - val_categorical_accuracy: 0.0818\n",
      "Epoch 9/40\n",
      "92/92 [==============================] - 11s 118ms/step - loss: 2.9510 - categorical_accuracy: 0.0985 - val_loss: 2.9602 - val_categorical_accuracy: 0.0818\n",
      "Epoch 10/40\n",
      "92/92 [==============================] - 11s 119ms/step - loss: 2.9516 - categorical_accuracy: 0.0985 - val_loss: 2.9591 - val_categorical_accuracy: 0.0818\n",
      "Epoch 11/40\n",
      "92/92 [==============================] - 11s 118ms/step - loss: 2.9516 - categorical_accuracy: 0.0985 - val_loss: 2.9591 - val_categorical_accuracy: 0.0818\n",
      "Epoch 12/40\n",
      "92/92 [==============================] - 11s 120ms/step - loss: 2.9516 - categorical_accuracy: 0.0985 - val_loss: 2.9591 - val_categorical_accuracy: 0.0818\n",
      "Epoch 13/40\n",
      "92/92 [==============================] - 11s 118ms/step - loss: 2.9517 - categorical_accuracy: 0.0965 - val_loss: 2.9605 - val_categorical_accuracy: 0.0818\n",
      "Epoch 14/40\n",
      "92/92 [==============================] - 11s 120ms/step - loss: 2.9514 - categorical_accuracy: 0.0985 - val_loss: 2.9589 - val_categorical_accuracy: 0.0818\n",
      "Epoch 15/40\n",
      "92/92 [==============================] - 11s 119ms/step - loss: 2.9518 - categorical_accuracy: 0.0955 - val_loss: 2.9590 - val_categorical_accuracy: 0.0818\n",
      "Epoch 16/40\n",
      "92/92 [==============================] - 11s 121ms/step - loss: 2.9523 - categorical_accuracy: 0.0958 - val_loss: 2.9603 - val_categorical_accuracy: 0.0818\n",
      "Epoch 17/40\n",
      "92/92 [==============================] - 11s 121ms/step - loss: 2.9515 - categorical_accuracy: 0.0985 - val_loss: 2.9593 - val_categorical_accuracy: 0.0818\n",
      "Epoch 18/40\n",
      "92/92 [==============================] - 11s 121ms/step - loss: 2.9515 - categorical_accuracy: 0.0944 - val_loss: 2.9598 - val_categorical_accuracy: 0.0818\n",
      "Epoch 19/40\n",
      "92/92 [==============================] - 11s 121ms/step - loss: 2.9518 - categorical_accuracy: 0.0985 - val_loss: 2.9595 - val_categorical_accuracy: 0.0818\n",
      "Epoch 20/40\n",
      "92/92 [==============================] - 11s 120ms/step - loss: 2.9521 - categorical_accuracy: 0.0941 - val_loss: 2.9584 - val_categorical_accuracy: 0.0818\n",
      "Epoch 21/40\n",
      "92/92 [==============================] - 11s 121ms/step - loss: 2.9515 - categorical_accuracy: 0.0985 - val_loss: 2.9594 - val_categorical_accuracy: 0.0818\n",
      "Epoch 22/40\n",
      "92/92 [==============================] - 11s 121ms/step - loss: 2.9515 - categorical_accuracy: 0.0948 - val_loss: 2.9605 - val_categorical_accuracy: 0.0818\n",
      "Epoch 23/40\n",
      "92/92 [==============================] - 11s 120ms/step - loss: 2.9518 - categorical_accuracy: 0.0985 - val_loss: 2.9597 - val_categorical_accuracy: 0.0818\n",
      "Epoch 24/40\n",
      "92/92 [==============================] - 11s 121ms/step - loss: 2.9522 - categorical_accuracy: 0.0951 - val_loss: 2.9598 - val_categorical_accuracy: 0.0818\n",
      "Epoch 25/40\n",
      "92/92 [==============================] - 11s 122ms/step - loss: 2.9516 - categorical_accuracy: 0.0985 - val_loss: 2.9586 - val_categorical_accuracy: 0.0818\n",
      "Epoch 26/40\n",
      "92/92 [==============================] - 11s 121ms/step - loss: 2.9520 - categorical_accuracy: 0.0897 - val_loss: 2.9604 - val_categorical_accuracy: 0.0818\n",
      "Epoch 27/40\n",
      "92/92 [==============================] - 11s 122ms/step - loss: 2.9521 - categorical_accuracy: 0.0985 - val_loss: 2.9591 - val_categorical_accuracy: 0.0818\n",
      "Epoch 28/40\n",
      "92/92 [==============================] - 11s 122ms/step - loss: 2.9516 - categorical_accuracy: 0.0924 - val_loss: 2.9606 - val_categorical_accuracy: 0.0818\n",
      "Epoch 29/40\n",
      "92/92 [==============================] - 11s 123ms/step - loss: 2.9519 - categorical_accuracy: 0.0985 - val_loss: 2.9611 - val_categorical_accuracy: 0.0818\n",
      "Epoch 30/40\n",
      "92/92 [==============================] - 11s 123ms/step - loss: 2.9517 - categorical_accuracy: 0.0985 - val_loss: 2.9609 - val_categorical_accuracy: 0.0818\n",
      "Epoch 31/40\n",
      "92/92 [==============================] - 11s 121ms/step - loss: 2.9518 - categorical_accuracy: 0.0985 - val_loss: 2.9603 - val_categorical_accuracy: 0.0818\n",
      "Epoch 32/40\n",
      "92/92 [==============================] - 11s 123ms/step - loss: 2.9523 - categorical_accuracy: 0.0985 - val_loss: 2.9604 - val_categorical_accuracy: 0.0818\n",
      "Epoch 33/40\n",
      "92/92 [==============================] - 11s 124ms/step - loss: 2.9518 - categorical_accuracy: 0.0958 - val_loss: 2.9590 - val_categorical_accuracy: 0.1005\n",
      "Epoch 34/40\n",
      "92/92 [==============================] - 11s 123ms/step - loss: 2.9517 - categorical_accuracy: 0.0972 - val_loss: 2.9601 - val_categorical_accuracy: 0.0818\n",
      "Epoch 35/40\n",
      "92/92 [==============================] - 11s 123ms/step - loss: 2.9518 - categorical_accuracy: 0.0985 - val_loss: 2.9590 - val_categorical_accuracy: 0.0818\n",
      "Epoch 36/40\n",
      "92/92 [==============================] - 11s 122ms/step - loss: 2.9518 - categorical_accuracy: 0.0985 - val_loss: 2.9598 - val_categorical_accuracy: 0.0818\n",
      "Epoch 37/40\n",
      "92/92 [==============================] - 11s 123ms/step - loss: 2.9518 - categorical_accuracy: 0.0979 - val_loss: 2.9598 - val_categorical_accuracy: 0.1005\n",
      "Epoch 38/40\n",
      "92/92 [==============================] - 11s 123ms/step - loss: 2.9521 - categorical_accuracy: 0.0917 - val_loss: 2.9593 - val_categorical_accuracy: 0.0818\n",
      "Epoch 39/40\n",
      "92/92 [==============================] - 11s 122ms/step - loss: 2.9517 - categorical_accuracy: 0.0965 - val_loss: 2.9592 - val_categorical_accuracy: 0.0818\n",
      "Epoch 40/40\n",
      "92/92 [==============================] - 11s 124ms/step - loss: 2.9520 - categorical_accuracy: 0.0985 - val_loss: 2.9598 - val_categorical_accuracy: 0.0818\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7b03d0708df0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "lstm.fit(X_train, y_train, batch_size=batch_size, epochs=max_epochs, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c83c56",
   "metadata": {},
   "source": [
    "<h1>Some Random Model<h1/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "67357f64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compiling the model\n",
      "Training the model\n",
      "Epoch 1/40\n",
      "92/92 [==============================] - 15s 105ms/step - loss: 2.5759 - categorical_accuracy: 0.2260 - val_loss: 2.1117 - val_categorical_accuracy: 0.3441\n",
      "Epoch 2/40\n",
      "92/92 [==============================] - 9s 96ms/step - loss: 2.1056 - categorical_accuracy: 0.3075 - val_loss: 1.8717 - val_categorical_accuracy: 0.4072\n",
      "Epoch 3/40\n",
      "92/92 [==============================] - 9s 93ms/step - loss: 2.0060 - categorical_accuracy: 0.3454 - val_loss: 1.8910 - val_categorical_accuracy: 0.3816\n",
      "Epoch 4/40\n",
      "92/92 [==============================] - 9s 96ms/step - loss: 1.8734 - categorical_accuracy: 0.3931 - val_loss: 1.8866 - val_categorical_accuracy: 0.3969\n",
      "Epoch 5/40\n",
      "92/92 [==============================] - 9s 95ms/step - loss: 1.7852 - categorical_accuracy: 0.4200 - val_loss: 1.6609 - val_categorical_accuracy: 0.4600\n",
      "Epoch 6/40\n",
      "92/92 [==============================] - 9s 97ms/step - loss: 1.7134 - categorical_accuracy: 0.4408 - val_loss: 1.6299 - val_categorical_accuracy: 0.4821\n",
      "Epoch 7/40\n",
      "92/92 [==============================] - 9s 97ms/step - loss: 1.6341 - categorical_accuracy: 0.4766 - val_loss: 1.6039 - val_categorical_accuracy: 0.4719\n",
      "Epoch 8/40\n",
      "92/92 [==============================] - 9s 98ms/step - loss: 1.5969 - categorical_accuracy: 0.4801 - val_loss: 1.6335 - val_categorical_accuracy: 0.4532\n",
      "Epoch 9/40\n",
      "92/92 [==============================] - 9s 96ms/step - loss: 1.5311 - categorical_accuracy: 0.4951 - val_loss: 1.5714 - val_categorical_accuracy: 0.4940\n",
      "Epoch 10/40\n",
      "92/92 [==============================] - 9s 98ms/step - loss: 1.5151 - categorical_accuracy: 0.5005 - val_loss: 1.5191 - val_categorical_accuracy: 0.5128\n",
      "Epoch 11/40\n",
      "92/92 [==============================] - 9s 99ms/step - loss: 1.4352 - categorical_accuracy: 0.5257 - val_loss: 1.5389 - val_categorical_accuracy: 0.4872\n",
      "Epoch 12/40\n",
      "92/92 [==============================] - 9s 99ms/step - loss: 1.4202 - categorical_accuracy: 0.5411 - val_loss: 1.4419 - val_categorical_accuracy: 0.5383\n",
      "Epoch 13/40\n",
      "92/92 [==============================] - 9s 98ms/step - loss: 1.4324 - categorical_accuracy: 0.5285 - val_loss: 1.5283 - val_categorical_accuracy: 0.5077\n",
      "Epoch 14/40\n",
      "92/92 [==============================] - 9s 99ms/step - loss: 1.3890 - categorical_accuracy: 0.5469 - val_loss: 1.5412 - val_categorical_accuracy: 0.5060\n",
      "Epoch 15/40\n",
      "92/92 [==============================] - 9s 98ms/step - loss: 1.3588 - categorical_accuracy: 0.5588 - val_loss: 1.4323 - val_categorical_accuracy: 0.5434\n",
      "Epoch 16/40\n",
      "92/92 [==============================] - 9s 100ms/step - loss: 1.2972 - categorical_accuracy: 0.5827 - val_loss: 1.4240 - val_categorical_accuracy: 0.5503\n",
      "Epoch 17/40\n",
      "92/92 [==============================] - 9s 102ms/step - loss: 1.3312 - categorical_accuracy: 0.5656 - val_loss: 1.6317 - val_categorical_accuracy: 0.4787\n",
      "Epoch 18/40\n",
      "92/92 [==============================] - 9s 102ms/step - loss: 1.3056 - categorical_accuracy: 0.5779 - val_loss: 1.4556 - val_categorical_accuracy: 0.5315\n",
      "Epoch 19/40\n",
      "92/92 [==============================] - 10s 107ms/step - loss: 1.3201 - categorical_accuracy: 0.5718 - val_loss: 1.4720 - val_categorical_accuracy: 0.5315\n",
      "Epoch 20/40\n",
      "92/92 [==============================] - 9s 99ms/step - loss: 1.2679 - categorical_accuracy: 0.5861 - val_loss: 1.3728 - val_categorical_accuracy: 0.5656\n",
      "Epoch 21/40\n",
      "92/92 [==============================] - 9s 101ms/step - loss: 1.2028 - categorical_accuracy: 0.6127 - val_loss: 1.4834 - val_categorical_accuracy: 0.5383\n",
      "Epoch 22/40\n",
      "92/92 [==============================] - 9s 103ms/step - loss: 1.1921 - categorical_accuracy: 0.6103 - val_loss: 1.4708 - val_categorical_accuracy: 0.5281\n",
      "Epoch 23/40\n",
      "92/92 [==============================] - 9s 99ms/step - loss: 1.2257 - categorical_accuracy: 0.6140 - val_loss: 1.4580 - val_categorical_accuracy: 0.5468\n",
      "Epoch 24/40\n",
      "92/92 [==============================] - 9s 102ms/step - loss: 1.2041 - categorical_accuracy: 0.6130 - val_loss: 1.3590 - val_categorical_accuracy: 0.5758\n",
      "Epoch 25/40\n",
      "92/92 [==============================] - 9s 101ms/step - loss: 1.1637 - categorical_accuracy: 0.6328 - val_loss: 1.4262 - val_categorical_accuracy: 0.5588\n",
      "Epoch 26/40\n",
      "92/92 [==============================] - 9s 102ms/step - loss: 1.0890 - categorical_accuracy: 0.6502 - val_loss: 1.3632 - val_categorical_accuracy: 0.5724\n",
      "Epoch 27/40\n",
      "92/92 [==============================] - 9s 101ms/step - loss: 1.1376 - categorical_accuracy: 0.6304 - val_loss: 1.3748 - val_categorical_accuracy: 0.5486\n",
      "Epoch 28/40\n",
      "92/92 [==============================] - 9s 100ms/step - loss: 1.0695 - categorical_accuracy: 0.6556 - val_loss: 1.4102 - val_categorical_accuracy: 0.5673\n",
      "Epoch 29/40\n",
      "92/92 [==============================] - 10s 103ms/step - loss: 1.0742 - categorical_accuracy: 0.6570 - val_loss: 1.3701 - val_categorical_accuracy: 0.5622\n",
      "Epoch 30/40\n",
      "92/92 [==============================] - 10s 104ms/step - loss: 1.0850 - categorical_accuracy: 0.6464 - val_loss: 1.3360 - val_categorical_accuracy: 0.5877\n",
      "Epoch 31/40\n",
      "92/92 [==============================] - 9s 102ms/step - loss: 1.0761 - categorical_accuracy: 0.6498 - val_loss: 1.4094 - val_categorical_accuracy: 0.5724\n",
      "Epoch 32/40\n",
      "92/92 [==============================] - 9s 100ms/step - loss: 1.1601 - categorical_accuracy: 0.6253 - val_loss: 1.4238 - val_categorical_accuracy: 0.5588\n",
      "Epoch 33/40\n",
      "92/92 [==============================] - 9s 102ms/step - loss: 1.1358 - categorical_accuracy: 0.6338 - val_loss: 1.3919 - val_categorical_accuracy: 0.5707\n",
      "Epoch 34/40\n",
      "92/92 [==============================] - 9s 97ms/step - loss: 1.0281 - categorical_accuracy: 0.6734 - val_loss: 1.3861 - val_categorical_accuracy: 0.5877\n",
      "Epoch 35/40\n",
      "92/92 [==============================] - 9s 94ms/step - loss: 1.0543 - categorical_accuracy: 0.6686 - val_loss: 1.6660 - val_categorical_accuracy: 0.5315\n",
      "Epoch 36/40\n",
      "92/92 [==============================] - 9s 94ms/step - loss: 1.1961 - categorical_accuracy: 0.6086 - val_loss: 1.3907 - val_categorical_accuracy: 0.5741\n",
      "Epoch 37/40\n",
      "92/92 [==============================] - 9s 93ms/step - loss: 1.1427 - categorical_accuracy: 0.6287 - val_loss: 1.3914 - val_categorical_accuracy: 0.5741\n",
      "Epoch 38/40\n",
      "92/92 [==============================] - 9s 94ms/step - loss: 1.0719 - categorical_accuracy: 0.6570 - val_loss: 1.4066 - val_categorical_accuracy: 0.5520\n",
      "Epoch 39/40\n",
      "92/92 [==============================] - 9s 93ms/step - loss: 1.0232 - categorical_accuracy: 0.6666 - val_loss: 1.3517 - val_categorical_accuracy: 0.5758\n",
      "Epoch 40/40\n",
      "92/92 [==============================] - 9s 96ms/step - loss: 1.0051 - categorical_accuracy: 0.6717 - val_loss: 1.4211 - val_categorical_accuracy: 0.5707\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7b03d1823130>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Conv1D, LSTM, Dense, Bidirectional, Dropout, GlobalAveragePooling1D\n",
    "CnnBLSTM = Sequential([\n",
    "    Conv1D(64, 3, activation='relu', input_shape=(sequence_length, num_input_values)),\n",
    "    Bidirectional(LSTM(128, return_sequences=True)),\n",
    "    Dropout(0.5),\n",
    "    Bidirectional(LSTM(128)),\n",
    "    Dropout(0.5),\n",
    "    Dense(actions.shape[0], activation='softmax')\n",
    "])\n",
    "\n",
    "print(\"compiling the model\")\n",
    "# Compile the model\n",
    "CnnBLSTM.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['categorical_accuracy'])\n",
    "\n",
    "print(\"Training the model\")\n",
    "# Train the model\n",
    "CnnBLSTM.fit(X_train, y_train, epochs=max_epochs, batch_size=32, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f58c4d8",
   "metadata": {},
   "source": [
    "## 6b. LSTM + Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "07591dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_block(inputs, time_steps):\n",
    "    \"\"\"\n",
    "    Attention layer for deep neural network\n",
    "    \n",
    "    \"\"\"\n",
    "    # Attention weights\n",
    "    a = Permute((2, 1))(inputs)\n",
    "    a = Dense(time_steps, activation='softmax')(a)\n",
    "    \n",
    "    # Attention vector\n",
    "    a_probs = Permute((2, 1), name='attention_vec')(a)\n",
    "    \n",
    "    # Luong's multiplicative score\n",
    "    output_attention_mul = multiply([inputs, a_probs], name='attention_mul') \n",
    "    \n",
    "    return output_attention_mul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c5c2e2e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 40, 132)]            0         []                            \n",
      "                                                                                                  \n",
      " bidirectional_2 (Bidirecti  (None, 40, 512)              796672    ['input_1[0][0]']             \n",
      " onal)                                                                                            \n",
      "                                                                                                  \n",
      " permute (Permute)           (None, 512, 40)              0         ['bidirectional_2[0][0]']     \n",
      "                                                                                                  \n",
      " dense_4 (Dense)             (None, 512, 40)              1640      ['permute[0][0]']             \n",
      "                                                                                                  \n",
      " attention_vec (Permute)     (None, 40, 512)              0         ['dense_4[0][0]']             \n",
      "                                                                                                  \n",
      " attention_mul (Multiply)    (None, 40, 512)              0         ['bidirectional_2[0][0]',     \n",
      "                                                                     'attention_vec[0][0]']       \n",
      "                                                                                                  \n",
      " flatten (Flatten)           (None, 20480)                0         ['attention_mul[0][0]']       \n",
      "                                                                                                  \n",
      " dense_5 (Dense)             (None, 512)                  1048627   ['flatten[0][0]']             \n",
      "                                                          2                                       \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)         (None, 512)                  0         ['dense_5[0][0]']             \n",
      "                                                                                                  \n",
      " dense_6 (Dense)             (None, 22)                   11286     ['dropout_2[0][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 11295870 (43.09 MB)\n",
      "Trainable params: 11295870 (43.09 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "HIDDEN_UNITS = 256\n",
    "\n",
    "# Input\n",
    "inputs = Input(shape=(sequence_length, num_input_values))\n",
    "\n",
    "# Bi-LSTM\n",
    "lstm_out = Bidirectional(LSTM(HIDDEN_UNITS, return_sequences=True))(inputs)\n",
    "\n",
    "# Attention\n",
    "attention_mul = attention_block(lstm_out, sequence_length)\n",
    "attention_mul = Flatten()(attention_mul)\n",
    "\n",
    "# Fully Connected Layer\n",
    "x = Dense(2*HIDDEN_UNITS, activation='relu')(attention_mul)\n",
    "x = Dropout(0.5)(x)\n",
    "\n",
    "# Output\n",
    "x = Dense(actions.shape[0], activation='softmax')(x)\n",
    "\n",
    "# Bring it all together\n",
    "AttnLSTM = Model(inputs=[inputs], outputs=x)\n",
    "print(AttnLSTM.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cf2f988d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "92/92 [==============================] - 23s 218ms/step - loss: 2.5579 - categorical_accuracy: 0.2131 - val_loss: 2.0462 - val_categorical_accuracy: 0.3560\n",
      "Epoch 2/40\n",
      "92/92 [==============================] - 19s 203ms/step - loss: 2.0800 - categorical_accuracy: 0.3103 - val_loss: 1.8846 - val_categorical_accuracy: 0.3714\n",
      "Epoch 3/40\n",
      "92/92 [==============================] - 19s 209ms/step - loss: 1.9204 - categorical_accuracy: 0.3628 - val_loss: 1.7911 - val_categorical_accuracy: 0.3935\n",
      "Epoch 4/40\n",
      "92/92 [==============================] - 19s 207ms/step - loss: 1.8040 - categorical_accuracy: 0.4020 - val_loss: 1.7376 - val_categorical_accuracy: 0.4344\n",
      "Epoch 5/40\n",
      "92/92 [==============================] - 19s 204ms/step - loss: 1.7461 - categorical_accuracy: 0.4197 - val_loss: 1.6862 - val_categorical_accuracy: 0.4497\n",
      "Epoch 6/40\n",
      "92/92 [==============================] - 20s 215ms/step - loss: 1.6706 - categorical_accuracy: 0.4426 - val_loss: 1.6312 - val_categorical_accuracy: 0.4736\n",
      "Epoch 7/40\n",
      "92/92 [==============================] - 20s 217ms/step - loss: 1.5784 - categorical_accuracy: 0.4821 - val_loss: 1.5734 - val_categorical_accuracy: 0.4872\n",
      "Epoch 8/40\n",
      "92/92 [==============================] - 19s 211ms/step - loss: 1.4947 - categorical_accuracy: 0.5121 - val_loss: 1.5463 - val_categorical_accuracy: 0.4804\n",
      "Epoch 9/40\n",
      "92/92 [==============================] - 19s 204ms/step - loss: 1.4731 - categorical_accuracy: 0.5131 - val_loss: 1.5810 - val_categorical_accuracy: 0.5060\n",
      "Epoch 10/40\n",
      "92/92 [==============================] - 18s 200ms/step - loss: 1.4095 - categorical_accuracy: 0.5435 - val_loss: 1.5176 - val_categorical_accuracy: 0.5094\n",
      "Epoch 11/40\n",
      "92/92 [==============================] - 18s 201ms/step - loss: 1.4125 - categorical_accuracy: 0.5247 - val_loss: 1.4858 - val_categorical_accuracy: 0.5077\n",
      "Epoch 12/40\n",
      "92/92 [==============================] - 19s 202ms/step - loss: 1.3496 - categorical_accuracy: 0.5513 - val_loss: 1.4862 - val_categorical_accuracy: 0.5128\n",
      "Epoch 13/40\n",
      "92/92 [==============================] - 19s 206ms/step - loss: 1.2750 - categorical_accuracy: 0.5810 - val_loss: 1.4507 - val_categorical_accuracy: 0.5247\n",
      "Epoch 14/40\n",
      "92/92 [==============================] - 19s 202ms/step - loss: 1.2702 - categorical_accuracy: 0.5878 - val_loss: 1.5305 - val_categorical_accuracy: 0.5128\n",
      "Epoch 15/40\n",
      "92/92 [==============================] - 19s 205ms/step - loss: 1.2859 - categorical_accuracy: 0.5861 - val_loss: 1.4905 - val_categorical_accuracy: 0.5230\n",
      "Epoch 16/40\n",
      "92/92 [==============================] - 19s 205ms/step - loss: 1.2397 - categorical_accuracy: 0.5936 - val_loss: 1.3959 - val_categorical_accuracy: 0.5605\n",
      "Epoch 17/40\n",
      "92/92 [==============================] - 19s 209ms/step - loss: 1.2000 - categorical_accuracy: 0.6120 - val_loss: 1.4043 - val_categorical_accuracy: 0.5690\n",
      "Epoch 18/40\n",
      "92/92 [==============================] - 19s 203ms/step - loss: 1.1581 - categorical_accuracy: 0.6294 - val_loss: 1.4175 - val_categorical_accuracy: 0.5520\n",
      "Epoch 19/40\n",
      "92/92 [==============================] - 19s 202ms/step - loss: 1.1806 - categorical_accuracy: 0.6144 - val_loss: 1.3876 - val_categorical_accuracy: 0.5503\n",
      "Epoch 20/40\n",
      "92/92 [==============================] - 19s 206ms/step - loss: 1.1317 - categorical_accuracy: 0.6308 - val_loss: 1.3551 - val_categorical_accuracy: 0.5741\n",
      "Epoch 21/40\n",
      "92/92 [==============================] - 19s 208ms/step - loss: 1.1070 - categorical_accuracy: 0.6413 - val_loss: 1.4402 - val_categorical_accuracy: 0.5571\n",
      "Epoch 22/40\n",
      "92/92 [==============================] - 19s 202ms/step - loss: 1.0910 - categorical_accuracy: 0.6393 - val_loss: 1.4432 - val_categorical_accuracy: 0.5622\n",
      "Epoch 23/40\n",
      "92/92 [==============================] - 19s 205ms/step - loss: 1.0891 - categorical_accuracy: 0.6509 - val_loss: 1.4430 - val_categorical_accuracy: 0.5434\n",
      "Epoch 24/40\n",
      "92/92 [==============================] - 19s 202ms/step - loss: 1.0668 - categorical_accuracy: 0.6522 - val_loss: 1.4387 - val_categorical_accuracy: 0.5588\n",
      "Epoch 25/40\n",
      "92/92 [==============================] - 19s 204ms/step - loss: 1.0729 - categorical_accuracy: 0.6635 - val_loss: 1.4031 - val_categorical_accuracy: 0.5588\n",
      "Epoch 26/40\n",
      "92/92 [==============================] - 19s 204ms/step - loss: 1.0505 - categorical_accuracy: 0.6570 - val_loss: 1.3346 - val_categorical_accuracy: 0.5826\n",
      "Epoch 27/40\n",
      "92/92 [==============================] - 19s 202ms/step - loss: 1.0686 - categorical_accuracy: 0.6512 - val_loss: 1.3642 - val_categorical_accuracy: 0.5843\n",
      "Epoch 28/40\n",
      "92/92 [==============================] - 19s 203ms/step - loss: 1.0318 - categorical_accuracy: 0.6686 - val_loss: 1.4315 - val_categorical_accuracy: 0.5741\n",
      "Epoch 29/40\n",
      "92/92 [==============================] - 19s 205ms/step - loss: 1.0459 - categorical_accuracy: 0.6642 - val_loss: 1.3670 - val_categorical_accuracy: 0.5963\n",
      "Epoch 30/40\n",
      "92/92 [==============================] - 19s 211ms/step - loss: 1.0284 - categorical_accuracy: 0.6686 - val_loss: 1.3884 - val_categorical_accuracy: 0.5826\n",
      "Epoch 31/40\n",
      "92/92 [==============================] - 32s 354ms/step - loss: 1.0050 - categorical_accuracy: 0.6734 - val_loss: 1.4574 - val_categorical_accuracy: 0.5656\n",
      "Epoch 32/40\n",
      "92/92 [==============================] - 34s 374ms/step - loss: 0.9876 - categorical_accuracy: 0.6816 - val_loss: 1.4368 - val_categorical_accuracy: 0.5673\n",
      "Epoch 33/40\n",
      "92/92 [==============================] - 30s 330ms/step - loss: 0.9698 - categorical_accuracy: 0.6901 - val_loss: 1.3890 - val_categorical_accuracy: 0.5860\n",
      "Epoch 34/40\n",
      "92/92 [==============================] - 23s 248ms/step - loss: 0.9501 - categorical_accuracy: 0.6966 - val_loss: 1.4137 - val_categorical_accuracy: 0.5928\n",
      "Epoch 35/40\n",
      "92/92 [==============================] - 21s 230ms/step - loss: 0.9838 - categorical_accuracy: 0.6850 - val_loss: 1.4297 - val_categorical_accuracy: 0.5945\n",
      "Epoch 36/40\n",
      "92/92 [==============================] - 20s 219ms/step - loss: 0.9927 - categorical_accuracy: 0.6812 - val_loss: 1.4696 - val_categorical_accuracy: 0.5690\n",
      "Epoch 37/40\n",
      "92/92 [==============================] - 20s 221ms/step - loss: 0.9689 - categorical_accuracy: 0.6884 - val_loss: 1.4386 - val_categorical_accuracy: 0.5843\n",
      "Epoch 38/40\n",
      "92/92 [==============================] - 20s 218ms/step - loss: 1.0026 - categorical_accuracy: 0.6727 - val_loss: 1.4296 - val_categorical_accuracy: 0.5928\n",
      "Epoch 39/40\n",
      "92/92 [==============================] - 21s 233ms/step - loss: 1.0360 - categorical_accuracy: 0.6683 - val_loss: 1.4366 - val_categorical_accuracy: 0.5639\n",
      "Epoch 40/40\n",
      "92/92 [==============================] - 21s 225ms/step - loss: 0.9842 - categorical_accuracy: 0.6764 - val_loss: 1.3802 - val_categorical_accuracy: 0.5997\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x731c61538040>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AttnLSTM.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "AttnLSTM.fit(X_train, y_train, batch_size=batch_size, epochs=max_epochs, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b89f67cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model map\n",
    "models = {\n",
    "    'LSTM': lstm, \n",
    "    'LSTM_Attention_128HUs': AttnLSTM, \n",
    "    'CNN_BLSTM model': CnnBLSTM,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a928f612",
   "metadata": {},
   "source": [
    "# 7a. Save Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0a7647ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dell/.local/lib/python3.10/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "for model_name, model in models.items():\n",
    "    save_dir = os.path.join(os.getcwd(), f\"{model_name}.h5\")\n",
    "    model.save(save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13fecf26",
   "metadata": {},
   "source": [
    "# 7b. Load Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ed0114a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run model rebuild before doing this\n",
    "for model_name, model in models.items():\n",
    "    load_dir = os.path.join(os.getcwd(), f\"{model_name}.h5\")\n",
    "    model.load_weights(load_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7747c6",
   "metadata": {},
   "source": [
    "# 8. Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2101a592",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in models.values():\n",
    "    res = model.predict(X_test, verbose=0)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b36c98e",
   "metadata": {},
   "source": [
    "# 9. Evaluations using Confusion Matrix and Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ecf242d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results = {}\n",
    "eval_results['confusion matrix'] = None\n",
    "eval_results['accuracy'] = None\n",
    "eval_results['precision'] = None\n",
    "eval_results['recall'] = None\n",
    "eval_results['f1 score'] = None\n",
    "\n",
    "confusion_matrices = {}\n",
    "classification_accuracies = {}   \n",
    "precisions = {}\n",
    "recalls = {}\n",
    "f1_scores = {} "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d6778f",
   "metadata": {},
   "source": [
    "## 9a. Confusion Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fccbb90f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM confusion matrix: \n",
      "[[[  0 357]\n",
      "  [  0  35]]\n",
      "\n",
      " [[360   0]\n",
      "  [ 32   0]]\n",
      "\n",
      " [[369   0]\n",
      "  [ 23   0]]\n",
      "\n",
      " [[364   0]\n",
      "  [ 28   0]]\n",
      "\n",
      " [[386   0]\n",
      "  [  6   0]]\n",
      "\n",
      " [[380   0]\n",
      "  [ 12   0]]\n",
      "\n",
      " [[381   0]\n",
      "  [ 11   0]]\n",
      "\n",
      " [[373   0]\n",
      "  [ 19   0]]\n",
      "\n",
      " [[360   0]\n",
      "  [ 32   0]]\n",
      "\n",
      " [[371   0]\n",
      "  [ 21   0]]\n",
      "\n",
      " [[383   0]\n",
      "  [  9   0]]\n",
      "\n",
      " [[377   0]\n",
      "  [ 15   0]]\n",
      "\n",
      " [[387   0]\n",
      "  [  5   0]]\n",
      "\n",
      " [[378   0]\n",
      "  [ 14   0]]\n",
      "\n",
      " [[357   0]\n",
      "  [ 35   0]]\n",
      "\n",
      " [[387   0]\n",
      "  [  5   0]]\n",
      "\n",
      " [[384   0]\n",
      "  [  8   0]]\n",
      "\n",
      " [[382   0]\n",
      "  [ 10   0]]\n",
      "\n",
      " [[372   0]\n",
      "  [ 20   0]]\n",
      "\n",
      " [[383   0]\n",
      "  [  9   0]]\n",
      "\n",
      " [[358   0]\n",
      "  [ 34   0]]\n",
      "\n",
      " [[383   0]\n",
      "  [  9   0]]]\n",
      "LSTM_Attention_128HUs confusion matrix: \n",
      "[[[243 114]\n",
      "  [ 17  18]]\n",
      "\n",
      " [[360   0]\n",
      "  [ 32   0]]\n",
      "\n",
      " [[186 183]\n",
      "  [ 13  10]]\n",
      "\n",
      " [[364   0]\n",
      "  [ 28   0]]\n",
      "\n",
      " [[386   0]\n",
      "  [  6   0]]\n",
      "\n",
      " [[380   0]\n",
      "  [ 12   0]]\n",
      "\n",
      " [[381   0]\n",
      "  [ 11   0]]\n",
      "\n",
      " [[373   0]\n",
      "  [ 19   0]]\n",
      "\n",
      " [[360   0]\n",
      "  [ 32   0]]\n",
      "\n",
      " [[371   0]\n",
      "  [ 21   0]]\n",
      "\n",
      " [[383   0]\n",
      "  [  9   0]]\n",
      "\n",
      " [[356  21]\n",
      "  [ 14   1]]\n",
      "\n",
      " [[380   7]\n",
      "  [  5   0]]\n",
      "\n",
      " [[378   0]\n",
      "  [ 14   0]]\n",
      "\n",
      " [[356   1]\n",
      "  [ 35   0]]\n",
      "\n",
      " [[387   0]\n",
      "  [  5   0]]\n",
      "\n",
      " [[353  31]\n",
      "  [  5   3]]\n",
      "\n",
      " [[382   0]\n",
      "  [ 10   0]]\n",
      "\n",
      " [[372   0]\n",
      "  [ 20   0]]\n",
      "\n",
      " [[380   3]\n",
      "  [  9   0]]\n",
      "\n",
      " [[358   0]\n",
      "  [ 34   0]]\n",
      "\n",
      " [[383   0]\n",
      "  [  9   0]]]\n",
      "CNN_BLSTM model confusion matrix: \n",
      "[[[354   3]\n",
      "  [ 23  12]]\n",
      "\n",
      " [[237 123]\n",
      "  [  6  26]]\n",
      "\n",
      " [[366   3]\n",
      "  [ 14   9]]\n",
      "\n",
      " [[362   2]\n",
      "  [ 14  14]]\n",
      "\n",
      " [[386   0]\n",
      "  [  2   4]]\n",
      "\n",
      " [[373   7]\n",
      "  [  5   7]]\n",
      "\n",
      " [[378   3]\n",
      "  [  4   7]]\n",
      "\n",
      " [[368   5]\n",
      "  [ 15   4]]\n",
      "\n",
      " [[360   0]\n",
      "  [ 23   9]]\n",
      "\n",
      " [[368   3]\n",
      "  [  9  12]]\n",
      "\n",
      " [[383   0]\n",
      "  [  3   6]]\n",
      "\n",
      " [[376   1]\n",
      "  [  5  10]]\n",
      "\n",
      " [[386   1]\n",
      "  [  2   3]]\n",
      "\n",
      " [[377   1]\n",
      "  [  4  10]]\n",
      "\n",
      " [[355   2]\n",
      "  [ 13  22]]\n",
      "\n",
      " [[385   2]\n",
      "  [  2   3]]\n",
      "\n",
      " [[379   5]\n",
      "  [  2   6]]\n",
      "\n",
      " [[377   5]\n",
      "  [  2   8]]\n",
      "\n",
      " [[366   6]\n",
      "  [  8  12]]\n",
      "\n",
      " [[379   4]\n",
      "  [  2   7]]\n",
      "\n",
      " [[355   3]\n",
      "  [ 22  12]]\n",
      "\n",
      " [[381   2]\n",
      "  [  1   8]]]\n"
     ]
    }
   ],
   "source": [
    "for model_name, model in models.items():\n",
    "    yhat = model.predict(X_test, verbose=0)\n",
    "    \n",
    "    # Get list of classification predictions\n",
    "    ytrue = np.argmax(y_test, axis=1).tolist()\n",
    "    yhat = np.argmax(yhat, axis=1).tolist()\n",
    "    \n",
    "    # Confusion matrix\n",
    "    confusion_matrices[model_name] = multilabel_confusion_matrix(ytrue, yhat)\n",
    "    print(f\"{model_name} confusion matrix: {os.linesep}{confusion_matrices[model_name]}\")\n",
    "\n",
    "# Collect results \n",
    "eval_results['confusion matrix'] = confusion_matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76c6dc5",
   "metadata": {},
   "source": [
    "## 9b. Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e36146f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM classification accuracy = 8.929%\n",
      "LSTM_Attention_128HUs classification accuracy = 8.163%\n",
      "CNN_BLSTM model classification accuracy = 53.827%\n"
     ]
    }
   ],
   "source": [
    "for model_name, model in models.items():\n",
    "    yhat = model.predict(X_test, verbose=0)\n",
    "    \n",
    "    # Get list of classification predictions\n",
    "    ytrue = np.argmax(y_test, axis=1).tolist()\n",
    "    yhat = np.argmax(yhat, axis=1).tolist()\n",
    "    \n",
    "    # Model accuracy\n",
    "    classification_accuracies[model_name] = accuracy_score(ytrue, yhat)    \n",
    "    print(f\"{model_name} classification accuracy = {round(classification_accuracies[model_name]*100,3)}%\")\n",
    "\n",
    "# Collect results \n",
    "eval_results['accuracy'] = classification_accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33efa73a",
   "metadata": {},
   "source": [
    "## 9c. Precision, Recall, and F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "35067c48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dell/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/dell/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/dell/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM weighted average precision = 0.008\n",
      "LSTM weighted average recall = 0.089\n",
      "LSTM weighted average f1-score = 0.015\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dell/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/dell/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/dell/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM_Attention_128HUs weighted average precision = 0.019\n",
      "LSTM_Attention_128HUs weighted average recall = 0.082\n",
      "LSTM_Attention_128HUs weighted average f1-score = 0.03\n",
      "\n",
      "CNN_BLSTM model weighted average precision = 0.741\n",
      "CNN_BLSTM model weighted average recall = 0.538\n",
      "CNN_BLSTM model weighted average f1-score = 0.575\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for model_name, model in models.items():\n",
    "    yhat = model.predict(X_test, verbose=0)\n",
    "    \n",
    "    # Get list of classification predictions\n",
    "    ytrue = np.argmax(y_test, axis=1).tolist()\n",
    "    yhat = np.argmax(yhat, axis=1).tolist()\n",
    "    \n",
    "    # Precision, recall, and f1 score\n",
    "    report = classification_report(ytrue, yhat, target_names=actions, output_dict=True)\n",
    "    \n",
    "    precisions[model_name] = report['weighted avg']['precision']\n",
    "    recalls[model_name] = report['weighted avg']['recall']\n",
    "    f1_scores[model_name] = report['weighted avg']['f1-score'] \n",
    "   \n",
    "    print(f\"{model_name} weighted average precision = {round(precisions[model_name],3)}\")\n",
    "    print(f\"{model_name} weighted average recall = {round(recalls[model_name],3)}\")\n",
    "    print(f\"{model_name} weighted average f1-score = {round(f1_scores[model_name],3)}\\n\")\n",
    "\n",
    "# Collect results \n",
    "eval_results['precision'] = precisions\n",
    "eval_results['recall'] = recalls\n",
    "eval_results['f1 score'] = f1_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d39476",
   "metadata": {},
   "source": [
    "# 10. Choose Model to Test in Real Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d72d0605",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AttnLSTM\n",
    "model_name = 'AttnLSTM'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0015ce",
   "metadata": {},
   "source": [
    "# 11. Calculate Joint Angles & Count Reps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f172932f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_angle(a,b,c):\n",
    "    \"\"\"\n",
    "    Computes 3D joint angle inferred by 3 keypoints and their relative positions to one another\n",
    "    \n",
    "    \"\"\"\n",
    "    a = np.array(a) # First\n",
    "    b = np.array(b) # Mid\n",
    "    c = np.array(c) # End\n",
    "    \n",
    "    radians = np.arctan2(c[1]-b[1], c[0]-b[0]) - np.arctan2(a[1]-b[1], a[0]-b[0])\n",
    "    angle = np.abs(radians*180.0/np.pi)\n",
    "    \n",
    "    if angle >180.0:\n",
    "        angle = 360-angle\n",
    "        \n",
    "    return angle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "26f357fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coordinates(landmarks, mp_pose, side, joint):\n",
    "    \"\"\"\n",
    "    Retrieves x and y coordinates of a particular keypoint from the pose estimation model\n",
    "         \n",
    "     Args:\n",
    "         landmarks: processed keypoints from the pose estimation model\n",
    "         mp_pose: Mediapipe pose estimation model\n",
    "         side: 'left' or 'right'. Denotes the side of the body of the landmark of interest.\n",
    "         joint: 'shoulder', 'elbow', 'wrist', 'hip', 'knee', or 'ankle'. Denotes which body joint is associated with the landmark of interest.\n",
    "    \n",
    "    \"\"\"\n",
    "    coord = getattr(mp_pose.PoseLandmark,side.upper()+\"_\"+joint.upper())\n",
    "    x_coord_val = landmarks[coord.value].x\n",
    "    y_coord_val = landmarks[coord.value].y\n",
    "    return [x_coord_val, y_coord_val]            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f11273cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def viz_joint_angle(image, angle, joint):\n",
    "    \"\"\"\n",
    "    Displays the joint angle value near the joint within the image frame\n",
    "    \n",
    "    \"\"\"\n",
    "    cv2.putText(image, str(int(angle)), \n",
    "                   tuple(np.multiply(joint, [640, 480]).astype(int)), \n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2, cv2.LINE_AA\n",
    "                        )\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b64050d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_reps(image, current_action, landmarks, mp_pose):\n",
    "    \"\"\"\n",
    "    Counts repetitions of each exercise. Global count and stage (i.e., state) variables are updated within this function.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    global curl_counter, press_counter, squat_counter, curl_stage, press_stage, squat_stage\n",
    "    \n",
    "    if current_action == 'curl':\n",
    "        # Get coords\n",
    "        shoulder = get_coordinates(landmarks, mp_pose, 'left', 'shoulder')\n",
    "        elbow = get_coordinates(landmarks, mp_pose, 'left', 'elbow')\n",
    "        wrist = get_coordinates(landmarks, mp_pose, 'left', 'wrist')\n",
    "        \n",
    "        # calculate elbow angle\n",
    "        angle = calculate_angle(shoulder, elbow, wrist)\n",
    "        \n",
    "        # curl counter logic\n",
    "        if angle < 30:\n",
    "            curl_stage = \"up\" \n",
    "        if angle > 140 and curl_stage =='up':\n",
    "            curl_stage=\"down\"  \n",
    "            curl_counter +=1\n",
    "        press_stage = None\n",
    "        squat_stage = None\n",
    "            \n",
    "        # Viz joint angle\n",
    "        viz_joint_angle(image, angle, elbow)\n",
    "        \n",
    "    elif current_action == 'press':\n",
    "        \n",
    "        # Get coords\n",
    "        shoulder = get_coordinates(landmarks, mp_pose, 'left', 'shoulder')\n",
    "        elbow = get_coordinates(landmarks, mp_pose, 'left', 'elbow')\n",
    "        wrist = get_coordinates(landmarks, mp_pose, 'left', 'wrist')\n",
    "\n",
    "        # Calculate elbow angle\n",
    "        elbow_angle = calculate_angle(shoulder, elbow, wrist)\n",
    "        \n",
    "        # Compute distances between joints\n",
    "        shoulder2elbow_dist = abs(math.dist(shoulder,elbow))\n",
    "        shoulder2wrist_dist = abs(math.dist(shoulder,wrist))\n",
    "        \n",
    "        # Press counter logic\n",
    "        if (elbow_angle > 130) and (shoulder2elbow_dist < shoulder2wrist_dist):\n",
    "            press_stage = \"up\"\n",
    "        if (elbow_angle < 50) and (shoulder2elbow_dist > shoulder2wrist_dist) and (press_stage =='up'):\n",
    "            press_stage='down'\n",
    "            press_counter += 1\n",
    "        curl_stage = None\n",
    "        squat_stage = None\n",
    "            \n",
    "        # Viz joint angle\n",
    "        viz_joint_angle(image, elbow_angle, elbow)\n",
    "        \n",
    "    elif current_action == 'squat':\n",
    "        # Get coords\n",
    "        # left side\n",
    "        left_shoulder = get_coordinates(landmarks, mp_pose, 'left', 'shoulder')\n",
    "        left_hip = get_coordinates(landmarks, mp_pose, 'left', 'hip')\n",
    "        left_knee = get_coordinates(landmarks, mp_pose, 'left', 'knee')\n",
    "        left_ankle = get_coordinates(landmarks, mp_pose, 'left', 'ankle')\n",
    "        # right side\n",
    "        right_shoulder = get_coordinates(landmarks, mp_pose, 'right', 'shoulder')\n",
    "        right_hip = get_coordinates(landmarks, mp_pose, 'right', 'hip')\n",
    "        right_knee = get_coordinates(landmarks, mp_pose, 'right', 'knee')\n",
    "        right_ankle = get_coordinates(landmarks, mp_pose, 'right', 'ankle')\n",
    "        \n",
    "        # Calculate knee angles\n",
    "        left_knee_angle = calculate_angle(left_hip, left_knee, left_ankle)\n",
    "        right_knee_angle = calculate_angle(right_hip, right_knee, right_ankle)\n",
    "        \n",
    "        # Calculate hip angles\n",
    "        left_hip_angle = calculate_angle(left_shoulder, left_hip, left_knee)\n",
    "        right_hip_angle = calculate_angle(right_shoulder, right_hip, right_knee)\n",
    "        \n",
    "        # Squat counter logic\n",
    "        thr = 165\n",
    "        if (left_knee_angle < thr) and (right_knee_angle < thr) and (left_hip_angle < thr) and (right_hip_angle < thr):\n",
    "            squat_stage = \"down\"\n",
    "        if (left_knee_angle > thr) and (right_knee_angle > thr) and (left_hip_angle > thr) and (right_hip_angle > thr) and (squat_stage =='down'):\n",
    "            squat_stage='up'\n",
    "            squat_counter += 1\n",
    "        curl_stage = None\n",
    "        press_stage = None\n",
    "            \n",
    "        # Viz joint angles\n",
    "        viz_joint_angle(image, left_knee_angle, left_knee)\n",
    "        viz_joint_angle(image, left_hip_angle, left_hip)\n",
    "        \n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5116ef6",
   "metadata": {},
   "source": [
    "# 12. Test in Real Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4775b75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob_viz(res, actions, input_frame, colors):\n",
    "    \"\"\"\n",
    "    This function displays the model prediction probability distribution over the set of exercise classes\n",
    "    as a horizontal bar graph\n",
    "    \n",
    "    \"\"\"\n",
    "    output_frame = input_frame.copy()\n",
    "    for num, prob in enumerate(res):        \n",
    "        cv2.rectangle(output_frame, (0,60+num*40), (int(prob*100), 90+num*40), colors[num], -1)\n",
    "        cv2.putText(output_frame, actions[num], (0, 85+num*40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2, cv2.LINE_AA)\n",
    "        \n",
    "    return output_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6332bf1a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mp_pose' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 29\u001b[0m\n\u001b[1;32m     26\u001b[0m out \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mVideoWriter(video_name, cv2\u001b[38;5;241m.\u001b[39mVideoWriter_fourcc(\u001b[38;5;241m*\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMJPG\u001b[39m\u001b[38;5;124m\"\u001b[39m), FPS, (WIDTH,HEIGHT))\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Set mediapipe model \u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mmp_pose\u001b[49m\u001b[38;5;241m.\u001b[39mPose(min_detection_confidence\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, min_tracking_confidence\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m pose:\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m cap\u001b[38;5;241m.\u001b[39misOpened():\n\u001b[1;32m     31\u001b[0m \n\u001b[1;32m     32\u001b[0m         \u001b[38;5;66;03m# Read feed\u001b[39;00m\n\u001b[1;32m     33\u001b[0m         ret, frame \u001b[38;5;241m=\u001b[39m cap\u001b[38;5;241m.\u001b[39mread()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mp_pose' is not defined"
     ]
    }
   ],
   "source": [
    "# 1. New detection variables\n",
    "sequence = []\n",
    "predictions = []\n",
    "res = []\n",
    "threshold = 0.5 # minimum confidence to classify as an action/exercise\n",
    "current_action = ''\n",
    "\n",
    "# Rep counter logic variables\n",
    "curl_counter = 0\n",
    "press_counter = 0\n",
    "squat_counter = 0\n",
    "curl_stage = None\n",
    "press_stage = None\n",
    "squat_stage = None\n",
    "\n",
    "# Camera object\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Video writer object that saves a video of the real time test\n",
    "fourcc = cv2.VideoWriter_fourcc('M','J','P','G') # video compression format\n",
    "HEIGHT = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)) # webcam video frame height\n",
    "WIDTH = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)) # webcam video frame width\n",
    "FPS = int(cap.get(cv2.CAP_PROP_FPS)) # webcam video fram rate \n",
    "\n",
    "video_name = os.path.join(os.getcwd(),f\"{model_name}_real_time_test.avi\")\n",
    "out = cv2.VideoWriter(video_name, cv2.VideoWriter_fourcc(*\"MJPG\"), FPS, (WIDTH,HEIGHT))\n",
    "\n",
    "# Set mediapipe model \n",
    "with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\n",
    "    while cap.isOpened():\n",
    "\n",
    "        # Read feed\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # Make detection\n",
    "        image, results = mediapipe_detection(frame, pose)\n",
    "        \n",
    "        # Draw landmarks\n",
    "        draw_landmarks(image, results)\n",
    "        \n",
    "        # 2. Prediction logic\n",
    "        keypoints = extract_keypoints(results)        \n",
    "        sequence.append(keypoints)      \n",
    "        sequence = sequence[-sequence_length:]\n",
    "              \n",
    "        if len(sequence) == sequence_length:\n",
    "            res = model.predict(np.expand_dims(sequence, axis=0), verbose=0)[0]           \n",
    "            predictions.append(np.argmax(res))\n",
    "            current_action = actions[np.argmax(res)]\n",
    "            confidence = np.max(res)\n",
    "            \n",
    "        #3. Viz logic\n",
    "            # Erase current action variable if no probability is above threshold\n",
    "            if confidence < threshold:\n",
    "                current_action = ''\n",
    "\n",
    "            # Viz probabilities\n",
    "            image = prob_viz(res, actions, image, colors)\n",
    "            \n",
    "            # Count reps\n",
    "            try:\n",
    "                landmarks = results.pose_landmarks.landmark\n",
    "                count_reps(\n",
    "                    image, current_action, landmarks, mp_pose)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            # Display graphical information\n",
    "            cv2.rectangle(image, (0,0), (640, 40), colors[np.argmax(res)], -1)\n",
    "            cv2.putText(image, 'curl ' + str(curl_counter), (3,30), \n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "            cv2.putText(image, 'press ' + str(press_counter), (240,30), \n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "            cv2.putText(image, 'squat ' + str(squat_counter), (490,30), \n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "         \n",
    "        # Show to screen\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "        \n",
    "        # Write to video file\n",
    "        if ret == True:\n",
    "            out.write(image)\n",
    "\n",
    "        # Break gracefully\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "af9980a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "80aa1d3f3a8cfb37a38c47373cc49a39149184c5fa770d709389b1b8782c1d85"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
